{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "5dd3e479",
   "metadata": {},
   "source": [
    "## First, cleaning the file that contains the inital material analysis of all service lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "c4faeb2b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file: all_files_initial_material.xlsx\n",
      "Successfully read file with 889624 rows\n",
      "Using columns: 'SITE ID' and 'STREET ADDRESS'\n",
      "\n",
      "Results:\n",
      "Total rows in file: 889624\n",
      "Unique SITE ID + STREET ADDRESS combinations: 815561\n",
      "Number of duplicate combinations: 72212\n",
      "Total rows that are duplicates: 146275\n",
      "Results saved to duplicate_analysis_results.xlsx\n",
      "Deduplicated file saved to all_files_initial_material_deduplicated.xlsx\n",
      "Original row count: 889624\n",
      "Deduplicated row count: 815561\n",
      "Rows removed: 74063\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "\n",
    "def find_duplicates_in_single_file(file_path):\n",
    "    \"\"\"\n",
    "    Identifies duplicate entries in all_files_initial_material.xlsx based on SITE ID and STREET ADDRESS.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the all_files_initial_material.xlsx file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the duplicate entries\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing file: {file_path}\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Successfully read file with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Find the SITE ID and STREET ADDRESS columns\n",
    "    site_id_col = None\n",
    "    street_address_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if isinstance(col, str):\n",
    "            if col.upper() == \"SITE ID\":\n",
    "                site_id_col = col\n",
    "            elif \"STREET ADDRESS\" in col.upper():\n",
    "                street_address_col = col\n",
    "    \n",
    "    if site_id_col is None or street_address_col is None:\n",
    "        print(\"Could not find SITE ID and/or STREET ADDRESS columns\")\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        return None\n",
    "    \n",
    "    print(f\"Using columns: '{site_id_col}' and '{street_address_col}'\")\n",
    "    \n",
    "    # Clean the data for comparison\n",
    "    df['SITE_ID_CLEAN'] = df[site_id_col].astype(str).str.strip().str.upper()\n",
    "    df['STREET_ADDRESS_CLEAN'] = df[street_address_col].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # Create a combined key for duplicate checking\n",
    "    df['SITE_ADDRESS_KEY'] = df['SITE_ID_CLEAN'] + '|' + df['STREET_ADDRESS_CLEAN']\n",
    "    \n",
    "    # Find duplicates based on the combined key\n",
    "    duplicates = df[df.duplicated(subset=['SITE_ADDRESS_KEY'], keep=False)].copy()\n",
    "    \n",
    "    # Sort by the key to group duplicates together\n",
    "    if not duplicates.empty:\n",
    "        duplicates = duplicates.sort_values('SITE_ADDRESS_KEY')\n",
    "    \n",
    "    # Count the number of duplicates\n",
    "    duplicate_count = duplicates['SITE_ADDRESS_KEY'].nunique()\n",
    "    total_duplicate_rows = len(duplicates)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Total rows in file: {len(df)}\")\n",
    "    print(f\"Unique SITE ID + STREET ADDRESS combinations: {df['SITE_ADDRESS_KEY'].nunique()}\")\n",
    "    print(f\"Number of duplicate combinations: {duplicate_count}\")\n",
    "    print(f\"Total rows that are duplicates: {total_duplicate_rows}\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def save_results_to_excel(duplicates, original_df, output_file, deduplicated_file):\n",
    "    \"\"\"\n",
    "    Saves the duplicate results to an Excel file and creates a deduplicated version\n",
    "    \n",
    "    Args:\n",
    "        duplicates (DataFrame): DataFrame containing duplicates\n",
    "        original_df (DataFrame): Original DataFrame with all data\n",
    "        output_file (str): Path to save the output Excel file\n",
    "        deduplicated_file (str): Path to save the deduplicated Excel file\n",
    "    \"\"\"\n",
    "    if duplicates is None or duplicates.empty:\n",
    "        print(\"No duplicates to save\")\n",
    "        original_df.to_excel(deduplicated_file, index=False)\n",
    "        print(f\"Original file saved as {deduplicated_file} (no duplicates found)\")\n",
    "        return\n",
    "    \n",
    "    # Prepare summary data\n",
    "    unique_combinations = duplicates['SITE_ADDRESS_KEY'].nunique()\n",
    "    summary_data = {\n",
    "        \"Metric\": [\n",
    "            \"Total duplicate combinations\",\n",
    "            \"Total duplicate rows\",\n",
    "            \"Original row count\",\n",
    "            \"Deduplicated row count\",\n",
    "            \"Rows removed\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            unique_combinations,\n",
    "            len(duplicates),\n",
    "            len(original_df),\n",
    "            len(original_df) - len(duplicates) + unique_combinations,\n",
    "            len(duplicates) - unique_combinations\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Keep only the original columns plus a duplicate group identifier\n",
    "    result_columns = [col for col in duplicates.columns if not col in \n",
    "                     ['SITE_ID_CLEAN', 'STREET_ADDRESS_CLEAN', 'SITE_ADDRESS_KEY']]\n",
    "    \n",
    "    # Add a duplicate group identifier\n",
    "    duplicates['Duplicate_Group'] = duplicates.groupby('SITE_ADDRESS_KEY').ngroup() + 1\n",
    "    \n",
    "    # Prepare the final dataframe for export\n",
    "    export_df = duplicates[result_columns + ['Duplicate_Group']]\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "        export_df.to_excel(writer, sheet_name=\"Duplicates\", index=False)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    # Create a deduplicated version of the file\n",
    "    # First, mark all rows that are part of duplicates\n",
    "    original_df['is_duplicate'] = original_df['SITE_ADDRESS_KEY'].isin(duplicates['SITE_ADDRESS_KEY'])\n",
    "    \n",
    "    # Get all rows that are not duplicates\n",
    "    unique_rows = original_df[~original_df['is_duplicate']]\n",
    "    \n",
    "    # For each duplicate group, keep only the first occurrence\n",
    "    duplicate_groups = duplicates['SITE_ADDRESS_KEY'].unique()\n",
    "    kept_duplicates = []\n",
    "    \n",
    "    for group_key in duplicate_groups:\n",
    "        # Get the first row from each group\n",
    "        first_row = original_df[original_df['SITE_ADDRESS_KEY'] == group_key].iloc[0]\n",
    "        kept_duplicates.append(first_row)\n",
    "    \n",
    "    # Combine unique rows with one representative from each duplicate group\n",
    "    deduplicated_df = pd.concat([unique_rows, pd.DataFrame(kept_duplicates)])\n",
    "    \n",
    "    # Drop the temporary columns used for deduplication\n",
    "    deduplicated_df = deduplicated_df.drop(columns=['SITE_ID_CLEAN', 'STREET_ADDRESS_CLEAN', \n",
    "                                                    'SITE_ADDRESS_KEY', 'is_duplicate'])\n",
    "    \n",
    "    # Save the deduplicated file\n",
    "    deduplicated_df.to_excel(deduplicated_file, index=False)\n",
    "    print(f\"Deduplicated file saved to {deduplicated_file}\")\n",
    "    print(f\"Original row count: {len(original_df)}\")\n",
    "    print(f\"Deduplicated row count: {len(deduplicated_df)}\")\n",
    "    print(f\"Rows removed: {len(original_df) - len(deduplicated_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the file paths\n",
    "    file_path = \"all_files_initial_material.xlsx\"\n",
    "    output_file = \"duplicate_analysis_results.xlsx\"\n",
    "    deduplicated_file = \"all_files_initial_material_deduplicated.xlsx\"\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = find_duplicates_in_single_file(file_path)\n",
    "    \n",
    "    # Read the original file again to create the deduplicated version\n",
    "    try:\n",
    "        original_df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Find the SITE ID and STREET ADDRESS columns\n",
    "        site_id_col = None\n",
    "        street_address_col = None\n",
    "        \n",
    "        for col in original_df.columns:\n",
    "            if isinstance(col, str):\n",
    "                if col.upper() == \"SITE ID\":\n",
    "                    site_id_col = col\n",
    "                elif \"STREET ADDRESS\" in col.upper():\n",
    "                    street_address_col = col\n",
    "        \n",
    "        # Clean the data for comparison (same as in the duplicates function)\n",
    "        original_df['SITE_ID_CLEAN'] = original_df[site_id_col].astype(str).str.strip().str.upper()\n",
    "        original_df['STREET_ADDRESS_CLEAN'] = original_df[street_address_col].astype(str).str.strip().str.upper()\n",
    "        original_df['SITE_ADDRESS_KEY'] = original_df['SITE_ID_CLEAN'] + '|' + original_df['STREET_ADDRESS_CLEAN']\n",
    "        \n",
    "        # Save results and create deduplicated file\n",
    "        save_results_to_excel(duplicates, original_df, output_file, deduplicated_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file for deduplication: {str(e)}\")\n",
    "        if duplicates is not None:\n",
    "            # Fall back to just saving the duplicates analysis\n",
    "            save_results_to_excel(duplicates, None, output_file, None)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ecdde98e",
   "metadata": {},
   "source": [
    "## Also finding the duplicates, this time for the file that has merged PSWID"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "cee82628",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Analyzing file: merged_output.xlsx\n",
      "Successfully read file with 857751 rows\n",
      "Using columns: 'SITE ID' and 'STREET ADDRESS'\n",
      "\n",
      "Results:\n",
      "Total rows in file: 857751\n",
      "Unique SITE ID + STREET ADDRESS combinations: 784959\n",
      "Number of duplicate combinations: 71730\n",
      "Total rows that are duplicates: 144522\n",
      "Results saved to duplicate_analysis_results_merged.xlsx\n",
      "Deduplicated file saved to merged_deduplicated.xlsx\n",
      "Original row count: 857751\n",
      "Deduplicated row count: 784959\n",
      "Rows removed: 72792\n"
     ]
    }
   ],
   "source": [
    "def find_duplicates_in_single_file(file_path):\n",
    "    \"\"\"\n",
    "    Identifies duplicate entries in all_files_initial_material.xlsx based on SITE ID and STREET ADDRESS.\n",
    "    \n",
    "    Args:\n",
    "        file_path (str): Path to the all_files_initial_material.xlsx file\n",
    "        \n",
    "    Returns:\n",
    "        DataFrame: DataFrame containing the duplicate entries\n",
    "    \"\"\"\n",
    "    print(f\"Analyzing file: {file_path}\")\n",
    "    \n",
    "    # Read the Excel file\n",
    "    try:\n",
    "        df = pd.read_excel(file_path)\n",
    "        print(f\"Successfully read file with {len(df)} rows\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error reading file: {str(e)}\")\n",
    "        return None\n",
    "    \n",
    "    # Find the SITE ID and STREET ADDRESS columns\n",
    "    site_id_col = None\n",
    "    street_address_col = None\n",
    "    \n",
    "    for col in df.columns:\n",
    "        if isinstance(col, str):\n",
    "            if col.upper() == \"SITE ID\":\n",
    "                site_id_col = col\n",
    "            elif \"STREET ADDRESS\" in col.upper():\n",
    "                street_address_col = col\n",
    "    \n",
    "    if site_id_col is None or street_address_col is None:\n",
    "        print(\"Could not find SITE ID and/or STREET ADDRESS columns\")\n",
    "        print(\"Available columns:\", df.columns.tolist())\n",
    "        return None\n",
    "    \n",
    "    print(f\"Using columns: '{site_id_col}' and '{street_address_col}'\")\n",
    "    \n",
    "    # Clean the data for comparison\n",
    "    df['SITE_ID_CLEAN'] = df[site_id_col].astype(str).str.strip().str.upper()\n",
    "    df['STREET_ADDRESS_CLEAN'] = df[street_address_col].astype(str).str.strip().str.upper()\n",
    "    \n",
    "    # Create a combined key for duplicate checking\n",
    "    df['SITE_ADDRESS_KEY'] = df['SITE_ID_CLEAN'] + '|' + df['STREET_ADDRESS_CLEAN']\n",
    "    \n",
    "    # Find duplicates based on the combined key\n",
    "    duplicates = df[df.duplicated(subset=['SITE_ADDRESS_KEY'], keep=False)].copy()\n",
    "    \n",
    "    # Sort by the key to group duplicates together\n",
    "    if not duplicates.empty:\n",
    "        duplicates = duplicates.sort_values('SITE_ADDRESS_KEY')\n",
    "    \n",
    "    # Count the number of duplicates\n",
    "    duplicate_count = duplicates['SITE_ADDRESS_KEY'].nunique()\n",
    "    total_duplicate_rows = len(duplicates)\n",
    "    \n",
    "    print(f\"\\nResults:\")\n",
    "    print(f\"Total rows in file: {len(df)}\")\n",
    "    print(f\"Unique SITE ID + STREET ADDRESS combinations: {df['SITE_ADDRESS_KEY'].nunique()}\")\n",
    "    print(f\"Number of duplicate combinations: {duplicate_count}\")\n",
    "    print(f\"Total rows that are duplicates: {total_duplicate_rows}\")\n",
    "    \n",
    "    return duplicates\n",
    "\n",
    "def save_results_to_excel(duplicates, original_df, output_file, deduplicated_file):\n",
    "    \"\"\"\n",
    "    Saves the duplicate results to an Excel file and creates a deduplicated version\n",
    "    \n",
    "    Args:\n",
    "        duplicates (DataFrame): DataFrame containing duplicates\n",
    "        original_df (DataFrame): Original DataFrame with all data\n",
    "        output_file (str): Path to save the output Excel file\n",
    "        deduplicated_file (str): Path to save the deduplicated Excel file\n",
    "    \"\"\"\n",
    "    if duplicates is None or duplicates.empty:\n",
    "        print(\"No duplicates to save\")\n",
    "        original_df.to_excel(deduplicated_file, index=False)\n",
    "        print(f\"Original file saved as {deduplicated_file} (no duplicates found)\")\n",
    "        return\n",
    "    \n",
    "    # Prepare summary data\n",
    "    unique_combinations = duplicates['SITE_ADDRESS_KEY'].nunique()\n",
    "    summary_data = {\n",
    "        \"Metric\": [\n",
    "            \"Total duplicate combinations\",\n",
    "            \"Total duplicate rows\",\n",
    "            \"Original row count\",\n",
    "            \"Deduplicated row count\",\n",
    "            \"Rows removed\"\n",
    "        ],\n",
    "        \"Value\": [\n",
    "            unique_combinations,\n",
    "            len(duplicates),\n",
    "            len(original_df),\n",
    "            len(original_df) - len(duplicates) + unique_combinations,\n",
    "            len(duplicates) - unique_combinations\n",
    "        ]\n",
    "    }\n",
    "    summary_df = pd.DataFrame(summary_data)\n",
    "    \n",
    "    # Keep only the original columns plus a duplicate group identifier\n",
    "    result_columns = [col for col in duplicates.columns if not col in \n",
    "                     ['SITE_ID_CLEAN', 'STREET_ADDRESS_CLEAN', 'SITE_ADDRESS_KEY']]\n",
    "    \n",
    "    # Add a duplicate group identifier\n",
    "    duplicates['Duplicate_Group'] = duplicates.groupby('SITE_ADDRESS_KEY').ngroup() + 1\n",
    "    \n",
    "    # Prepare the final dataframe for export\n",
    "    export_df = duplicates[result_columns + ['Duplicate_Group']]\n",
    "    \n",
    "    # Save to Excel\n",
    "    with pd.ExcelWriter(output_file) as writer:\n",
    "        summary_df.to_excel(writer, sheet_name=\"Summary\", index=False)\n",
    "        export_df.to_excel(writer, sheet_name=\"Duplicates\", index=False)\n",
    "    \n",
    "    print(f\"Results saved to {output_file}\")\n",
    "    \n",
    "    # Create a deduplicated version of the file\n",
    "    # First, mark all rows that are part of duplicates\n",
    "    original_df['is_duplicate'] = original_df['SITE_ADDRESS_KEY'].isin(duplicates['SITE_ADDRESS_KEY'])\n",
    "    \n",
    "    # Get all rows that are not duplicates\n",
    "    unique_rows = original_df[~original_df['is_duplicate']]\n",
    "    \n",
    "    # For each duplicate group, keep only the first occurrence\n",
    "    duplicate_groups = duplicates['SITE_ADDRESS_KEY'].unique()\n",
    "    kept_duplicates = []\n",
    "    \n",
    "    for group_key in duplicate_groups:\n",
    "        # Get the first row from each group\n",
    "        first_row = original_df[original_df['SITE_ADDRESS_KEY'] == group_key].iloc[0]\n",
    "        kept_duplicates.append(first_row)\n",
    "    \n",
    "    # Combine unique rows with one representative from each duplicate group\n",
    "    deduplicated_df = pd.concat([unique_rows, pd.DataFrame(kept_duplicates)])\n",
    "    \n",
    "    # Drop the temporary columns used for deduplication\n",
    "    deduplicated_df = deduplicated_df.drop(columns=['SITE_ID_CLEAN', 'STREET_ADDRESS_CLEAN', \n",
    "                                                    'SITE_ADDRESS_KEY', 'is_duplicate'])\n",
    "    \n",
    "    # Save the deduplicated file\n",
    "    deduplicated_df.to_excel(deduplicated_file, index=False)\n",
    "    print(f\"Deduplicated file saved to {deduplicated_file}\")\n",
    "    print(f\"Original row count: {len(original_df)}\")\n",
    "    print(f\"Deduplicated row count: {len(deduplicated_df)}\")\n",
    "    print(f\"Rows removed: {len(original_df) - len(deduplicated_df)}\")\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    # Set the file paths\n",
    "    file_path = \"merged_output.xlsx\"\n",
    "    output_file = \"duplicate_analysis_results_merged.xlsx\"\n",
    "    deduplicated_file = \"merged_deduplicated.xlsx\"\n",
    "    \n",
    "    # Find duplicates\n",
    "    duplicates = find_duplicates_in_single_file(file_path)\n",
    "    \n",
    "    # Read the original file again to create the deduplicated version\n",
    "    try:\n",
    "        original_df = pd.read_excel(file_path)\n",
    "        \n",
    "        # Find the SITE ID and STREET ADDRESS columns\n",
    "        site_id_col = None\n",
    "        street_address_col = None\n",
    "        \n",
    "        for col in original_df.columns:\n",
    "            if isinstance(col, str):\n",
    "                if col.upper() == \"SITE ID\":\n",
    "                    site_id_col = col\n",
    "                elif \"STREET ADDRESS\" in col.upper():\n",
    "                    street_address_col = col\n",
    "        \n",
    "        # Clean the data for comparison (same as in the duplicates function)\n",
    "        original_df['SITE_ID_CLEAN'] = original_df[site_id_col].astype(str).str.strip().str.upper()\n",
    "        original_df['STREET_ADDRESS_CLEAN'] = original_df[street_address_col].astype(str).str.strip().str.upper()\n",
    "        original_df['SITE_ADDRESS_KEY'] = original_df['SITE_ID_CLEAN'] + '|' + original_df['STREET_ADDRESS_CLEAN']\n",
    "        \n",
    "        # Save results and create deduplicated file\n",
    "        save_results_to_excel(duplicates, original_df, output_file, deduplicated_file)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing file for deduplication: {str(e)}\")\n",
    "        if duplicates is not None:\n",
    "            # Fall back to just saving the duplicates analysis\n",
    "            save_results_to_excel(duplicates, None, output_file, None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a32cbecb",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
