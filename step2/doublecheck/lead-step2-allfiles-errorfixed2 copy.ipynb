{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Reading the files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def read_inventory_files(folder_path, sheet_pattern, columns_to_extract, output_filename):\n",
    "    \"\"\"\n",
    "    Read all rows properly from lead service line inventory Excel files\n",
    "    \n",
    "    Args:\n",
    "        folder_path: Path to folder containing Excel files\n",
    "        sheet_pattern: Text pattern to identify the correct sheet\n",
    "        columns_to_extract: List of column names to extract\n",
    "        output_filename: Name for the output file\n",
    "    \"\"\"\n",
    "    print(f\"Processing {sheet_pattern} data...\")\n",
    "    all_data = []\n",
    "    \n",
    "    for file_name in os.listdir(folder_path):\n",
    "        if not file_name.endswith(('.xlsx', '.xls')) or file_name.startswith('~$'):\n",
    "            continue\n",
    "            \n",
    "        file_path = os.path.join(folder_path, file_name)\n",
    "        print(f\"Reading: {file_name}\")\n",
    "        \n",
    "        try:\n",
    "            # Get all sheet names\n",
    "            xl = pd.ExcelFile(file_path, engine=\"openpyxl\")\n",
    "            \n",
    "            # Find sheet that contains the pattern (case-insensitive)\n",
    "            sheet_name = None\n",
    "            for sheet in xl.sheet_names:\n",
    "                # More flexible matching - case-insensitive and allowing for partial matches\n",
    "                if sheet_pattern.lower() in sheet.lower():\n",
    "                    sheet_name = sheet\n",
    "                    break\n",
    "            \n",
    "            if not sheet_name:\n",
    "                # Try alternative sheet names if \"Updated Material Inventory\" not found\n",
    "                alternative_patterns = [\"material inventory\", \"inventory\", \"updated inventory\", \"material\"]\n",
    "                for alt_pattern in alternative_patterns:\n",
    "                    for sheet in xl.sheet_names:\n",
    "                        if alt_pattern in sheet.lower():\n",
    "                            sheet_name = sheet\n",
    "                            print(f\"  - Using alternative sheet: '{sheet}' instead of '{sheet_pattern}'\")\n",
    "                            break\n",
    "                    if sheet_name:\n",
    "                        break\n",
    "                        \n",
    "                if not sheet_name:\n",
    "                    print(f\"  - No matching sheet in {file_name}\")\n",
    "                    continue\n",
    "            \n",
    "            # First, try to find the header row\n",
    "            # Read more rows to locate headers\n",
    "            sample = pd.read_excel(file_path, sheet_name=sheet_name, nrows=30, header=None)\n",
    "\n",
    "            # Try to find a row containing our key columns\n",
    "            header_row = None\n",
    "            header_scores = []\n",
    "\n",
    "            for i in range(min(30, len(sample))):\n",
    "                row = sample.iloc[i].astype(str).str.upper()\n",
    "                # Check if this row contains several of our column names\n",
    "                matches = 0\n",
    "                keywords = [\"SITE\", \"ID\", \"LOCATION\", \"ADDRESS\", \"MATERIAL\", \"STREET\", \"TOWN\"]\n",
    "                \n",
    "                for keyword in keywords:\n",
    "                    if any(keyword in str(val) for val in row):\n",
    "                        matches += 1\n",
    "                        \n",
    "                header_scores.append((i, matches))\n",
    "                \n",
    "                # If we find enough matches, use this as header\n",
    "                if matches >= 3:\n",
    "                    header_row = i\n",
    "                    break\n",
    "\n",
    "            # If we didn't find a clear winner, use the row with highest score\n",
    "            if header_row is None and header_scores:\n",
    "                header_scores.sort(key=lambda x: x[1], reverse=True)\n",
    "                if header_scores[0][1] >= 2:  # At least 2 matches\n",
    "                    header_row = header_scores[0][0]\n",
    "                    print(f\"  - Using best header candidate at row {header_row} with score {header_scores[0][1]}\")\n",
    "            \n",
    "            # If we found a header row, use it\n",
    "            if header_row is not None:\n",
    "                # Now read the full data with correct header\n",
    "                df = pd.read_excel(\n",
    "                    file_path, \n",
    "                    sheet_name=sheet_name, \n",
    "                    header=header_row,\n",
    "                    keep_default_na=True,\n",
    "                    # Expanded list of error values to catch\n",
    "                    na_values=['#REF!', '#N/A', '#NAME?', '#DIV/0!', '#NULL!', '#NUM!', '#VALUE!', '#ERROR!']\n",
    "                )\n",
    "                \n",
    "                # Standardize column names (uppercase)\n",
    "                df.columns = df.columns.str.upper().str.strip()\n",
    "                \n",
    "                # IMPORTANT: Fix for numeric/unnamed columns\n",
    "                # Replace any numeric column names with descriptive names\n",
    "                numeric_cols = [col for col in df.columns if isinstance(col, (int, float)) or str(col).isdigit()]\n",
    "                for col in numeric_cols:\n",
    "                    df = df.rename(columns={col: f\"UNNAMED_{col}\"})\n",
    "                \n",
    "                # Debug: print all available columns\n",
    "                print(f\"  - Available columns: {df.columns.tolist()}\")\n",
    "\n",
    "                # NEW: Check for and handle SITE ID = 0 or SITE ID containing \"#REF!\"\n",
    "                site_id_col = next((col for col in df.columns if \"SITE ID\" in col.upper()), None)\n",
    "                if site_id_col:\n",
    "                    # Convert the column to string for text operations\n",
    "                    df[site_id_col] = df[site_id_col].astype(str)\n",
    "                    \n",
    "                    # 1. Handle any remaining errors that weren't caught by na_values\n",
    "                    error_patterns = [\"#REF\", \"#ERROR\", \"#VALUE\", \"#NULL\", \"#DIV\", \"#NAME\", \"#N/A\", \"#NUM\"]\n",
    "                    error_mask = df[site_id_col].apply(lambda x: any(err in str(x) for err in error_patterns))\n",
    "                    \n",
    "                    if error_mask.any():\n",
    "                        print(f\"  - Removing {error_mask.sum()} rows with error values in SITE ID\")\n",
    "                        df.loc[error_mask, site_id_col] = \"\"  # Just blank out the error cell\n",
    "                        # Optionally add a flag column\n",
    "                        df[\"HAS_SITE_ID_ERROR\"] = error_mask\n",
    "                    \n",
    "                    # 2. Handle zeros in SITE ID that might be from #REF! errors\n",
    "                    zero_mask = (df[site_id_col] == \"0\") | (df[site_id_col] == \"0.0\")\n",
    "                    if zero_mask.any():\n",
    "                        # Check if these are likely error rows based on other columns being empty\n",
    "                        empty_rows = df.loc[zero_mask].drop(columns=[site_id_col]).isna().all(axis=1)\n",
    "                        \n",
    "                        if empty_rows.any():\n",
    "                            print(f\"  - Removing {empty_rows.sum()} rows with SITE ID = 0 and all other columns empty\")\n",
    "                            # Keep rows where SITE ID = 0 but other data exists\n",
    "                            df = df[~(zero_mask & empty_rows)]  # Deletes these rows\n",
    "                        else:\n",
    "                            print(f\"  - Found {zero_mask.sum()} rows with SITE ID = 0, but other columns have data\")\n",
    "                    \n",
    "                    # NEW: Filter to only keep rows where Site ID is visible (has associated data)\n",
    "                    # Check if other key fields have data to indicate this is a \"real\" row\n",
    "                    location_col = next((col for col in df.columns if \"LOCATION\" in col.upper()), None)\n",
    "                    address_col = next((col for col in df.columns if \"ADDRESS\" in col.upper()), None)\n",
    "                    town_col = next((col for col in df.columns if \"TOWN\" in col.upper()), None)\n",
    "                    \n",
    "                    # Build criteria based on available columns\n",
    "                    criteria = []\n",
    "                    if location_col:\n",
    "                        criteria.append(df[location_col].notna())\n",
    "                    if address_col:\n",
    "                        criteria.append(df[address_col].notna() & (df[address_col].astype(str) != \"\"))\n",
    "                    if town_col:\n",
    "                        criteria.append(df[town_col].notna() & (df[town_col].astype(str) != \"\"))\n",
    "                    \n",
    "                    # If we have at least one criterion, apply filtering\n",
    "                    if criteria:\n",
    "                        has_associated_data = criteria[0]\n",
    "                        for crit in criteria[1:]:\n",
    "                            has_associated_data = has_associated_data | crit\n",
    "                        \n",
    "                        # Keep only rows where site ID is populated AND at least one key field has data\n",
    "                        visible_rows = df[site_id_col].notna() & has_associated_data\n",
    "                        invisible_rows = df[site_id_col].notna() & ~has_associated_data\n",
    "                        \n",
    "                        if invisible_rows.any():\n",
    "                            print(f\"  - Removing {invisible_rows.sum()} rows with invisible site IDs (no associated data)\")\n",
    "                            df[\"IS_INVISIBLE_SITE_ID\"] = invisible_rows| ~df[site_id_col].notna()\n",
    "\n",
    "                # NEW: Check if we need to combine STREET NUMBER and STREET NAME into STREET ADDRESS\n",
    "                has_street_number = any(\"STREET NUMBER\" in str(col).upper() for col in df.columns)\n",
    "                has_street_name = any(\"STREET NAME\" in str(col).upper() for col in df.columns)\n",
    "                has_street_address = any(\"STREET ADDRESS\" in str(col).upper() for col in df.columns)\n",
    "                \n",
    "                # Only combine if we have number+name but no address\n",
    "                if has_street_number and has_street_name and not has_street_address:\n",
    "                    print(f\"  - File has separate STREET NUMBER and STREET NAME columns, will combine into STREET ADDRESS\")\n",
    "                    \n",
    "                    # Find the exact column names for STREET NUMBER and STREET NAME\n",
    "                    street_number_col = next((col for col in df.columns if \"STREET NUMBER\" in str(col).upper()), None)\n",
    "                    street_name_col = next((col for col in df.columns if \"STREET NAME\" in str(col).upper()), None)\n",
    "                    \n",
    "                    if street_number_col and street_name_col:\n",
    "                        # Convert to string and handle NaN values\n",
    "                        df[street_number_col] = df[street_number_col].fillna('').astype(str)\n",
    "                        df[street_name_col] = df[street_name_col].fillna('').astype(str) \n",
    "                        \n",
    "                        # Combine the columns into a new STREET ADDRESS column\n",
    "                        df[\"STREET ADDRESS\"] = df[street_number_col] + \" \" + df[street_name_col]\n",
    "                        # Clean up any double spaces\n",
    "                        df[\"STREET ADDRESS\"] = df[\"STREET ADDRESS\"].str.replace(\"  \", \" \").str.strip()\n",
    "                        print(f\"  - Created STREET ADDRESS column by combining {street_number_col} and {street_name_col}\")\n",
    "                \n",
    "                # Prepare to filter and map columns\n",
    "                cols_to_keep = []\n",
    "                col_mapping = {}\n",
    "                \n",
    "                # MODIFIED: Keep track of which target columns we've already matched\n",
    "                # This prevents multiple columns mapping to the same target\n",
    "                matched_targets = set()\n",
    "\n",
    "                for col in df.columns:\n",
    "                    # Skip unnamed columns completely\n",
    "                    if \"UNNAMED\" in str(col).upper() or str(col).isdigit():\n",
    "                        continue\n",
    "                        \n",
    "                    # Check if this column approximately matches any of our target columns\n",
    "                    for target_col in columns_to_extract:\n",
    "                        # Skip targets we've already matched\n",
    "                        if target_col in matched_targets:\n",
    "                            continue\n",
    "                            \n",
    "                        # SPECIAL CASE: If we created a STREET ADDRESS column and this target is STREET ADDRESS,\n",
    "                        # we should only match our newly created column\n",
    "                        if target_col == \"STREET ADDRESS\" and \"STREET ADDRESS\" in df.columns:\n",
    "                            if col == \"STREET ADDRESS\":\n",
    "                                print(f\"  - Using newly created '{col}' for target '{target_col}'\")\n",
    "                                cols_to_keep.append(col)\n",
    "                                col_mapping[col] = target_col\n",
    "                                matched_targets.add(target_col)\n",
    "                            # Skip all other potential matches for STREET ADDRESS\n",
    "                            continue\n",
    "                        \n",
    "                        # If target is STREET NUMBER or STREET NAME but we've already handled STREET ADDRESS,\n",
    "                        # don't match these separately\n",
    "                        if (target_col == \"STREET NUMBER\" or target_col == \"STREET NAME\") and \"STREET ADDRESS\" in matched_targets:\n",
    "                            continue\n",
    "                        \n",
    "                        # Create variations of target column names to improve matching\n",
    "                        target_variations = [\n",
    "                            target_col,\n",
    "                            target_col.upper(),\n",
    "                            target_col.replace(\" \", \"\"),\n",
    "                            target_col.replace(\"IDENTIFIER\", \"ID\"),\n",
    "                            target_col.replace(\"ADDRESS\", \"ADDR\"),\n",
    "                            # Common abbreviations for address-related fields\n",
    "                            \"STREET\" if target_col == \"STREET ADDRESS\" else None,\n",
    "                            \"ST NUM\" if target_col == \"STREET NUMBER\" else None,\n",
    "                            \"ST NAME\" if target_col == \"STREET NAME\" else None,\n",
    "                            \"LOC ID\" if target_col == \"LOCATION IDENTIFIER\" else None\n",
    "                        ]\n",
    "                        target_variations = [v for v in target_variations if v]  # Remove None values\n",
    "                        \n",
    "                        # Check for various matching patterns\n",
    "                        matched = False\n",
    "                        \n",
    "                        # Check exact match\n",
    "                        if col == target_col or col == target_col.upper():\n",
    "                            matched = True\n",
    "                        # Check if column contains target or vice versa\n",
    "                        elif any(var in col for var in target_variations) or any(col in var for var in target_variations):\n",
    "                            matched = True\n",
    "                        # Check normalized versions (no spaces)\n",
    "                        elif any(var.replace(\" \", \"\") == col.replace(\" \", \"\") for var in target_variations):\n",
    "                            matched = True\n",
    "                            \n",
    "                        if matched:\n",
    "                            print(f\"  - Matched column '{col}' to target '{target_col}'\")\n",
    "                            cols_to_keep.append(col)\n",
    "                            col_mapping[col] = target_col\n",
    "                            matched_targets.add(target_col)\n",
    "                            break\n",
    "\n",
    "                # If important columns are missing, look again with more flexible matching\n",
    "                key_columns = [\"SITE ID\", \"LOCATION IDENTIFIER\", \"STREET ADDRESS\"]\n",
    "                missing_keys = [col for col in key_columns if col not in matched_targets]\n",
    "\n",
    "                if missing_keys:\n",
    "                    print(f\"  - Missing key columns: {missing_keys}, trying more flexible matching\")\n",
    "                    \n",
    "                    for missing_col in missing_keys:\n",
    "                        # Define more specific keywords for each missing column\n",
    "                        if missing_col == \"SITE ID\":\n",
    "                            keywords = [\"SITE\", \"ID\", \"FACILITY\", \"FAC ID\", \"FACILITY ID\", \"BUILDING ID\"]\n",
    "                        elif missing_col == \"LOCATION IDENTIFIER\":\n",
    "                            keywords = [\"LOCATION\", \"LOC\", \"POSITION\", \"PLACE\", \"IDENT\"]\n",
    "                        elif missing_col == \"STREET ADDRESS\":\n",
    "                            keywords = [\"ADDRESS\", \"ADDR\", \"LOCATION\", \"STREET\", \"ST ADDR\", \"PROPERTY\"]\n",
    "                            \n",
    "                        # Check all available columns again\n",
    "                        for col in df.columns:\n",
    "                            # Skip already matched columns\n",
    "                            if col in cols_to_keep:\n",
    "                                continue\n",
    "                                \n",
    "                            if any(keyword.upper() in col.upper() for keyword in keywords):\n",
    "                                print(f\"  - Flexible match: '{col}' to '{missing_col}'\")\n",
    "                                cols_to_keep.append(col)\n",
    "                                col_mapping[col] = missing_col\n",
    "                                matched_targets.add(missing_col)\n",
    "                                break\n",
    "                \n",
    "                if cols_to_keep:\n",
    "                    # Keep only matched columns\n",
    "                    df = df[cols_to_keep].copy()\n",
    "                    \n",
    "                    # Rename to standard names\n",
    "                    df = df.rename(columns=col_mapping)\n",
    "                    \n",
    "                    # Add source file column\n",
    "                    df[\"SOURCE FILE\"] = file_name\n",
    "                    \n",
    "                    # Drop rows where all values (except SOURCE FILE) are missing\n",
    "                    df = df.dropna(subset=list(set(col_mapping.values())), how='all')\n",
    "                    \n",
    "                    # Handle header rows that might have been read as data\n",
    "                    # If a row has text that matches column names, it's likely a header row\n",
    "                    headers_in_data = []\n",
    "                    for i, row in df.iterrows():\n",
    "                        row_values = row.astype(str).str.upper()\n",
    "                        if sum(any(col.upper() in val for val in row_values) \n",
    "                              for col in [\"SITE ID\", \"LOCATION\", \"ADDRESS\", \"MATERIAL\"]) >= 2:\n",
    "                            headers_in_data.append(i)\n",
    "                    \n",
    "                    # Drop identified header rows from data\n",
    "                    if headers_in_data:\n",
    "                        df = df.drop(headers_in_data)\n",
    "                    \n",
    "                    # Debug: check how many non-empty values we have for key columns\n",
    "                    for key_col in [\"SITE ID\", \"LOCATION IDENTIFIER\", \"STREET ADDRESS\"]:\n",
    "                        if key_col in df.columns:\n",
    "                            non_empty = df[key_col].astype(str).str.strip().str.len() > 0\n",
    "                            print(f\"  - {key_col}: {non_empty.sum()} non-empty values out of {len(df)}\")\n",
    "                    \n",
    "                    # Append to our collection\n",
    "                    all_data.append(df)\n",
    "                    print(f\"  - Added {len(df)} rows\")\n",
    "                else:\n",
    "                    print(f\"  - No matching columns found in {file_name}\")\n",
    "                \n",
    "            else:\n",
    "                print(f\"  - Could not identify header row in {file_name}\")\n",
    "                \n",
    "        except Exception as e:\n",
    "            print(f\"  - Error processing {file_name}: {e}\")\n",
    "            # Print more detailed error information\n",
    "            import traceback\n",
    "            print(f\"  - Error details: {traceback.format_exc()}\")\n",
    "    \n",
    "    # Combine all data\n",
    "    if all_data:\n",
    "        # Combine all dataframes\n",
    "        print(f\"Combining data from {len(all_data)} files...\")\n",
    "        \n",
    "        # Ensure all dataframes have the same column structure before combining\n",
    "        all_columns = []\n",
    "        for df in all_data:\n",
    "            all_columns.extend(df.columns.tolist())\n",
    "        \n",
    "        unique_columns = list(dict.fromkeys([col for col in all_columns \n",
    "                                            if col in columns_to_extract or col == \"SOURCE FILE\"]))\n",
    "        \n",
    "        for i in range(len(all_data)):\n",
    "            # Add missing columns\n",
    "            for col in unique_columns:\n",
    "                if col not in all_data[i].columns:\n",
    "                    all_data[i][col] = None\n",
    "                    \n",
    "            # Ensure dataframe only has our expected columns\n",
    "            all_data[i] = all_data[i][unique_columns]\n",
    "        \n",
    "        # Combine and save\n",
    "        final_df = pd.concat(all_data, ignore_index=True)\n",
    "        \n",
    "        # Replace any \"nan\" or numeric values in string columns with empty strings\n",
    "        for col in final_df.columns:\n",
    "            # Skip SOURCE FILE column\n",
    "            if col == \"SOURCE FILE\":\n",
    "                continue\n",
    "                \n",
    "            # For string columns, clean up nan values\n",
    "            if final_df[col].dtype == 'object':\n",
    "                # Replace nan, NULL, None etc with empty string\n",
    "                final_df[col] = final_df[col].astype(str)\n",
    "                final_df[col] = final_df[col].replace({\n",
    "                    \"nan\": \"\", \n",
    "                    \"None\": \"\", \n",
    "                    \"NaN\": \"\", \n",
    "                    \"NULL\": \"\", \n",
    "                    \"null\": \"\",\n",
    "                    \"#REF!\": \"\",\n",
    "                    \"#ERROR!\": \"\",\n",
    "                    \"#VALUE!\": \"\",\n",
    "                    \"#NAME?\": \"\",\n",
    "                    \"#N/A\": \"\",\n",
    "                    \"#DIV/0!\": \"\",\n",
    "                    \"#NUM!\": \"\"\n",
    "                })\n",
    "                \n",
    "                # MODIFIED: Only clean nan values in SITE ID, but keep numeric values\n",
    "                if col == \"SITE ID\":\n",
    "                    # First, check if this is a zero in SITE ID coming from a #REF! error\n",
    "                    zero_mask = (final_df[col] == \"0\")\n",
    "                    \n",
    "                    # If most of the SITE IDs are zeros, they're likely errors from #REF!\n",
    "                    if zero_mask.mean() > 0.5:  # If more than 50% are zeros\n",
    "                        print(f\"  - WARNING: Found {zero_mask.sum()} zeros in SITE ID column, treating as errors\")\n",
    "                        \n",
    "                        # For rows where SITE ID is 0, check if other columns have data\n",
    "                        empty_rows = final_df.loc[zero_mask].drop(columns=[col, \"SOURCE FILE\"]).apply(\n",
    "                            lambda x: all(val == \"\" for val in x), axis=1\n",
    "                        )\n",
    "                        \n",
    "                        if empty_rows.any():\n",
    "                            print(f\"  - Removing {empty_rows.sum()} rows with SITE ID = 0 and all other columns empty\")\n",
    "                            final_df = final_df[~(zero_mask & empty_rows)]\n",
    "                    \n",
    "                    # Normal cleaning of nan values\n",
    "                    final_df[col] = final_df[col].apply(\n",
    "                        lambda x: \"\" if x in [\n",
    "                            \"nan\", \"None\", \"NaN\", \"NULL\", \"null\", \n",
    "                            \"#REF!\", \"#ERROR!\", \"#VALUE!\", \"#NAME?\", \"#N/A\", \"#DIV/0!\", \"#NUM!\"\n",
    "                        ] else x\n",
    "                    )\n",
    "        \n",
    "        # Drop completely empty rows\n",
    "        final_df = final_df.loc[~final_df.drop(columns=[\"SOURCE FILE\"]).apply(\n",
    "            lambda x: all(val == \"\" for val in x), axis=1\n",
    "        )]\n",
    "        \n",
    "        # Save to Excel\n",
    "        output_path = os.path.join(folder_path, output_filename)\n",
    "        final_df.to_excel(output_path, index=False, engine=\"openpyxl\")\n",
    "        print(f\"Saved {len(final_df)} rows to {output_path}\")\n",
    "        \n",
    "        return final_df\n",
    "    else:\n",
    "        print(\"No data found\")\n",
    "        return None"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# STEP 2: Process Updated Material Inventory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define the folder path for \"test\" and \"test2\" folders\n",
    "\n",
    "folder_path = \"test2\"   # Using the all_files folder\n",
    "\n",
    "def process_updated_inventory():\n",
    "    \"\"\"Process Updated Material Inventory files\"\"\"\n",
    "    print(\"\\n\" + \"=\"*50)\n",
    "    print(\"PROCESSING UPDATED MATERIAL INVENTORY\")\n",
    "    print(\"=\"*50)\n",
    "    \n",
    "    # Define columns to extract for updated inventory\n",
    "    updated_columns = [\n",
    "        \"SITE ID\",\n",
    "        \"LOCATION IDENTIFIER\",\n",
    "        \"STREET ADDRESS\",\n",
    "        \"STREET NAME\",        \n",
    "        \"STREET NUMBER\",      \n",
    "        \"TOWN\",\n",
    "        \"STATE\",\n",
    "        \"ENTIRE SERVICE LINE MATERIAL CLASSIFICATION\"\n",
    "    ]\n",
    "    \n",
    "    # Process updated inventory files\n",
    "    print(f\"\\nProcessing folder: {folder_path}\")\n",
    "    updated_df = read_inventory_files(\n",
    "        folder_path=folder_path,\n",
    "        sheet_pattern=\"Updated Material Inventory\",  # Will now match partially and case-insensitively\n",
    "        columns_to_extract=updated_columns,\n",
    "        output_filename=\"updated_material_inventory.xlsx\"\n",
    "    )\n",
    "    \n",
    "    # Print summary of key columns to verify data quality\n",
    "    if updated_df is not None:\n",
    "        print(\"\\nData Quality Summary:\")\n",
    "        total_rows = len(updated_df)\n",
    "        for col in [\"SITE ID\", \"LOCATION IDENTIFIER\", \"STREET ADDRESS\", \"STREET NAME\", \"STREET NUMBER\"]:\n",
    "            if col in updated_df.columns:\n",
    "                non_empty = updated_df[col].astype(str).str.strip().str.len() > 0\n",
    "                count = non_empty.sum()\n",
    "                percent = (count / total_rows * 100) if total_rows > 0 else 0\n",
    "                print(f\"  - {col}: {count}/{total_rows} non-empty values ({percent:.1f}%)\")\n",
    "    \n",
    "    print(\"Updated Material Inventory processing complete!\")\n",
    "    return updated_df"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Running both steps"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "==================================================\n",
      "PROCESSING UPDATED MATERIAL INVENTORY\n",
      "==================================================\n",
      "\n",
      "Processing folder: test2\n",
      "Processing Updated Material Inventory data...\n",
      "Reading: CT0640011_LCRR_Inventory_Initial_10.16.24_East Hartford CTDPH copy.xlsx\n",
      "  - Available columns: ['SITE ID', 'LOCATION IDENTIFIER', 'STREET ADDRESS', 'TOWN', 'STATE', 'INITIAL CONNECTOR OR GOOSENECK MATERIAL CODE', 'INITIAL CONNECTOR OR GOOSENECK MATERIAL', 'CONNECTOR OR GOOSENECK VERIFIED MATERIAL CODE', 'CONNECTOR OR GOOSENECK VERIFIED MATERIAL', 'CONNECTOR OR GOOSENECK VERIFICATION SOURCE', 'CONNECTOR OR GOOSENECK VERIFICATION DATE', 'LEAD GOOSENECK, PIGTAIL, OR CONNECTOR REPLACEMENT DATE', 'LEAD GOOSENECK, PIGTAIL, OR CONNECTOR REPLACEMENT MATERIAL', 'NEW CONNECTOR OR GOOSENECK DIAMETER', 'INITIAL MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL CODE', 'INITIAL MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL CODE', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFICATION SOURCE', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL DATE', 'IS MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT REQUIRED?', 'CUSTOMER INITIATED MAIN SIDE OF CURB STOP LSLR DATE', 'CUSTOMER INITIATED MAIN SIDE OF CURB STOP LSLR EXTENSION DEADLINE', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT DATE', 'HIDDEN SYSTEM REPLACEMENT YEAR', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT MATERIAL CODE', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT MATERIAL', 'NEW MAIN SIDE OF CURB STOP SERVICE LINE DIAMETER', 'INITIAL BUILDING SIDE SERVICE LINE MATERIAL CODE', 'INITIAL BUILDING SIDE SERVICE LINE MATERIAL', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL CODE', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL', 'BUILDING SIDE SERVICE LINE VERIFICATION SOURCE', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL DATE', 'IS BUILDING SIDE STOP SERVICE LINE REPLACEMENT REQUIRED?', 'CUSTOMER REFUSALS / NON-RESPONSES FOR LSLR', 'BUILDING SIDE SERVICE LINE REPLACEMENT DATE', 'HIDDEN CUSTOMER REPLACEMENT YEAR', 'BUILDING SIDE SERVICE LINE REPLACEMENT MATERIAL CODE', 'BUILDING SIDE SERVICE LINE REPLACEMENT MATERIAL', 'NEW BUILDING SIDE SERVICE LINE MATERIAL DIAMETER', 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION', 'BUILDING PLUMBING VERIFIED MATERIAL', 'BUILDING PLUMBING VERIFIED MATERIAL2', 'POINT-OF-ENTRY OR POINT-OF-USE TREATMENT PRESENT?', 'BUILDING PLUMBING VERIFICATION DATE', 'BUILDING PLUMBING VERIFICATION SOURCE', 'LCRR SAMPLE SITE', 'UPDATED LCRR SAMPLE SITE TIER LEVEL', 'LCRR SAMPLING POINT ID', 'COMMENTS']\n",
      "  - Removing 5994 rows with SITE ID = 0 and all other columns empty\n",
      "  - Removing 14004 rows with invisible site IDs (no associated data)\n",
      "  - Matched column 'SITE ID' to target 'SITE ID'\n",
      "  - Matched column 'LOCATION IDENTIFIER' to target 'LOCATION IDENTIFIER'\n",
      "  - Using newly created 'STREET ADDRESS' for target 'STREET ADDRESS'\n",
      "  - Matched column 'TOWN' to target 'TOWN'\n",
      "  - Matched column 'STATE' to target 'STATE'\n",
      "  - Matched column 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION' to target 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION'\n",
      "  - SITE ID: 14006 non-empty values out of 14006\n",
      "  - LOCATION IDENTIFIER: 14006 non-empty values out of 14006\n",
      "  - STREET ADDRESS: 14006 non-empty values out of 14006\n",
      "  - Added 14006 rows\n",
      "Reading: test2_updated_material.xlsx\n",
      "  - No matching sheet in test2_updated_material.xlsx\n",
      "Reading: CT0340011_LCRR_Inventory_Initial_11-04-2024 copy.xlsx\n",
      "  - Using alternative sheet: 'Initial Material Inventory' instead of 'Updated Material Inventory'\n",
      "  - Available columns: ['SITE ID', 'LOCATION IDENTIFIER', 'STREET ADDRESS', 'TOWN', 'STATE', 'DISADVANTAGED NEIGHBORHOOD?', 'CONNECTOR OR GOOSENECK MATERIAL CODE', 'CONNECTOR OR GOOSENECK MATERIAL', 'CONNECTOR OR GOOSENECK MATERIAL INSTALL YEAR', 'WAS CONNECTOR OR GOOSENECK PREVIOUSLY LEAD?', 'MAIN SIDE OF CURB STOP SERVICE LINE OWNERSHIP', 'MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL CODE\\n(INITIAL INVENTORY ONLY)', 'MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL', 'WAS MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL EVER PREVIOUSLY LEAD?', 'MAIN SIDE OF CURB STOP SERVICE LINE DIAMETER (INCHES)', 'MAIN SIDE OF CURB STOP SERVICE LINE INSTALL YEAR', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFICATION DATE', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFICATION SOURCE', 'IS MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT REQUIRED?', 'BUILDING SIDE OF SERVICE LINE OWNERSHIP', 'BUILDING SIDE SERVICE LINE MATERIAL CODE\\n(INITIAL INVENTORY ONLY)', 'BUILDING SIDE SERVICE LINE MATERIAL', 'BUILDING SIDE SERVICE LINE DIAMETER (INCHES)', 'BUILDING SIDE SERVICE LINE INSTALL YEAR', 'BUILDING SIDE SERVICE LINE VERIFICATION DATE', 'BUILDING SIDE SERVICE LINE VERIFICATION SOURCE', 'IS BUILDING SIDE SERVICE LINE REPLACEMENT REQUIRED?', 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION', 'IS THERE LEAD SOLDER IN SERVICE LINE?', 'IS THERE OTHER FITTINGS AND/OR EQUIPMENT CONNECTED TO THE SERVICE LINE THAT CONTAIN LEAD?', 'BUILDING TYPE', '# OF SERVICE CONNECTIONS SERVED BY SERVICE LINE?', 'POINT-OF-ENTRY OR POINT-OF-USE TREATMENT PRESENT?', 'BUILDING PLUMBING MATERIAL 1', 'BUILDING PLUMBING MATERIAL 2', 'BUILDING PLUMBING MATERIAL INSTALLED DATE', 'BUILDING PLUMBING VERIFICATION DATE', 'BUILDING PLUMBING VERIFICATION SOURCE', 'CURRENT LCR SAMPLING SITE?', 'LCRR SAMPLE SITE', 'LCRR SAMPLE SITE TIER LEVEL', 'LCRR SAMPLING POINT ID', 'COMMENTS']\n",
      "  - Matched column 'SITE ID' to target 'SITE ID'\n",
      "  - Matched column 'LOCATION IDENTIFIER' to target 'LOCATION IDENTIFIER'\n",
      "  - Using newly created 'STREET ADDRESS' for target 'STREET ADDRESS'\n",
      "  - Matched column 'TOWN' to target 'TOWN'\n",
      "  - Matched column 'STATE' to target 'STATE'\n",
      "  - Matched column 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION' to target 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION'\n",
      "  - SITE ID: 11688 non-empty values out of 11688\n",
      "  - LOCATION IDENTIFIER: 11688 non-empty values out of 11688\n",
      "  - STREET ADDRESS: 11688 non-empty values out of 11688\n",
      "  - Added 11688 rows\n",
      "Reading: CT0070021_LCRR_Inventory_Initial_10.7.24 copy.xlsx\n",
      "  - Available columns: ['SITE ID', 'LOCATION IDENTIFIER', 'STREET ADDRESS', 'TOWN', 'STATE', 'INITIAL CONNECTOR OR GOOSENECK MATERIAL CODE', 'INITIAL CONNECTOR OR GOOSENECK MATERIAL', 'CONNECTOR OR GOOSENECK VERIFIED MATERIAL CODE', 'CONNECTOR OR GOOSENECK VERIFIED MATERIAL', 'CONNECTOR OR GOOSENECK VERIFICATION SOURCE', 'CONNECTOR OR GOOSENECK VERIFICATION DATE', 'LEAD GOOSENECK, PIGTAIL, OR CONNECTOR REPLACEMENT DATE', 'LEAD GOOSENECK, PIGTAIL, OR CONNECTOR REPLACEMENT MATERIAL', 'NEW CONNECTOR OR GOOSENECK DIAMETER', 'INITIAL MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL CODE', 'INITIAL MAIN SIDE OF CURB STOP SERVICE LINE MATERIAL', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL CODE', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFICATION SOURCE', 'MAIN SIDE OF CURB STOP SERVICE LINE VERIFIED MATERIAL DATE', 'IS MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT REQUIRED?', 'CUSTOMER INITIATED MAIN SIDE OF CURB STOP LSLR DATE', 'CUSTOMER INITIATED MAIN SIDE OF CURB STOP LSLR EXTENSION DEADLINE', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT DATE', 'HIDDEN SYSTEM REPLACEMENT YEAR', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT MATERIAL CODE', 'MAIN SIDE OF CURB STOP SERVICE LINE REPLACEMENT MATERIAL', 'NEW MAIN SIDE OF CURB STOP SERVICE LINE DIAMETER', 'INITIAL BUILDING SIDE SERVICE LINE MATERIAL CODE', 'INITIAL BUILDING SIDE SERVICE LINE MATERIAL', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL CODE', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL', 'BUILDING SIDE SERVICE LINE VERIFICATION SOURCE', 'BUILDING SIDE SERVICE LINE VERIFIED MATERIAL DATE', 'IS BUILDING SIDE STOP SERVICE LINE REPLACEMENT REQUIRED?', 'CUSTOMER REFUSALS / NON-RESPONSES FOR LSLR', 'BUILDING SIDE SERVICE LINE REPLACEMENT DATE', 'HIDDEN CUSTOMER REPLACEMENT YEAR', 'BUILDING SIDE SERVICE LINE REPLACEMENT MATERIAL CODE', 'BUILDING SIDE SERVICE LINE REPLACEMENT MATERIAL', 'NEW BUILDING SIDE SERVICE LINE MATERIAL DIAMETER', 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION', 'BUILDING PLUMBING VERIFIED MATERIAL', 'BUILDING PLUMBING VERIFIED MATERIAL2', 'POINT-OF-ENTRY OR POINT-OF-USE TREATMENT PRESENT?', 'BUILDING PLUMBING VERIFICATION DATE', 'BUILDING PLUMBING VERIFICATION SOURCE', 'LCRR SAMPLE SITE', 'UPDATED LCRR SAMPLE SITE TIER LEVEL', 'LCRR SAMPLING POINT ID', 'COMMENTS']\n",
      "  - Removing 17226 rows with invisible site IDs (no associated data)\n",
      "  - Matched column 'SITE ID' to target 'SITE ID'\n",
      "  - Matched column 'LOCATION IDENTIFIER' to target 'LOCATION IDENTIFIER'\n",
      "  - Using newly created 'STREET ADDRESS' for target 'STREET ADDRESS'\n",
      "  - Matched column 'TOWN' to target 'TOWN'\n",
      "  - Matched column 'STATE' to target 'STATE'\n",
      "  - Matched column 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION' to target 'ENTIRE SERVICE LINE MATERIAL CLASSIFICATION'\n",
      "  - SITE ID: 20000 non-empty values out of 20000\n",
      "  - LOCATION IDENTIFIER: 20000 non-empty values out of 20000\n",
      "  - STREET ADDRESS: 20000 non-empty values out of 20000\n",
      "  - Added 20000 rows\n",
      "Combining data from 3 files...\n",
      "Saved 23475 rows to test2/updated_material_inventory.xlsx\n",
      "\n",
      "Data Quality Summary:\n",
      "  - SITE ID: 20696/23475 non-empty values (88.2%)\n",
      "  - LOCATION IDENTIFIER: 11689/23475 non-empty values (49.8%)\n",
      "  - STREET ADDRESS: 14463/23475 non-empty values (61.6%)\n",
      "Updated Material Inventory processing complete!\n",
      "\n",
      "Processing complete!\n"
     ]
    }
   ],
   "source": [
    "# Execute the function\n",
    "if __name__ == \"__main__\":\n",
    "    # Run process\n",
    "    updated_df = process_updated_inventory()\n",
    "    print(\"\\nProcessing complete!\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
